{"cells":[{"metadata":{},"cell_type":"markdown","source":"# Laboratorium 4\n\nCelem czwartego laboratorium jest zapoznanie się oraz zaimplementowanie algorytmów głębokiego uczenia aktywnego. Zaimplementowane algorytmy będą testowane z wykorzystaniem wcześniej przygotowanych środowisk: *FrozenLake* i *Pacman* oraz środowiska z OpenAI - *CartPole*.\n"},{"metadata":{},"cell_type":"markdown","source":"Dołączenie standardowych bibliotek"},{"metadata":{"trusted":true},"cell_type":"code","source":"from collections import deque\nimport gym\nimport numpy as np\nimport random","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Dołączenie bibliotek ze środowiskami:"},{"metadata":{"trusted":true},"cell_type":"code","source":"from env.FrozenLakeMDP import frozenLake\nfrom env.FrozenLakeMDPExtended import frozenLakeExtended\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Dołączenie bibliotek do obsługi sieci neuronowych"},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Zadanie 1 - Deep Q-Network\n\n<p style='text-align: justify;'>\nCelem ćwiczenie jest zaimplementowanie algorytmu Deep Q-Network. Wartoscią oczekiwaną sieci jest:\n\\begin{equation}\n        Q(s_t, a_t) = r_{t+1} + \\gamma \\text{max}_a Q(s_{t + 1}, a)\n\\end{equation}\n</p>"},{"metadata":{"trusted":true},"cell_type":"code","source":"class DQNAgent:\n    def __init__(self, action_size, learning_rate, model):\n        self.action_size = action_size\n        self.memory = deque(maxlen=2000)\n        self.gamma = 0.95    # discount rate\n        self.epsilon = 1.0  # exploration rate\n        self.epsilon_min = 0.01\n        self.epsilon_decay = 0.95\n        self.learning_rate = learning_rate\n        self.model = model\n\n    def remember(self, state, action, reward, next_state, done):\n        #Function adds information to the memory about last action and its results\n        self.memory.append((state, action, reward, next_state, done)) \n\n    def get_action(self, state):\n        \"\"\"\n        Compute the action to take in the current state, including exploration.\n        With probability self.epsilon, we should take a random action.\n            otherwise - the best policy action (self.get_best_action).\n\n        Note: To pick randomly from a list, use random.choice(list).\n              To pick True or False with a given probablity, generate uniform number in [0, 1]\n              and compare it with your probability\n        \"\"\"\n\n        #\n        # INSERT CODE HERE to get action in a given state (according to epsilon greedy algorithm)\n        #        \n        \n        return chosen_action\n\n  \n    def get_best_action(self, state):\n        \"\"\"\n        Compute the best action to take in a state.\n        \"\"\"\n\n        #\n        # INSERT CODE HERE to get best possible action in a given state (remember to break ties randomly)\n        #\n\n        return best_action\n\n    def replay(self, batch_size):\n        \"\"\"\n        Function learn network using randomly selected actions from the memory. \n        First calculates Q value for the next state and choose action with the biggest value.\n        Target value is calculated according to:\n                Q(s,a) := (r + gamma * max_a(Q(s', a)))\n        except the situation when the next action is the last action, in such case Q(s, a) := r.\n        In order to change only those weights responsible for chosing given action, the rest values should be those\n        returned by the network for state state.\n        The network should be trained on batch_size samples.\n        Also every time the function replay is called self.epsilon value should be updated according to equation:\n        self.epsilon *= self.epsilon_decay\n        \"\"\"\n        #\n        # INSERT CODE HERE to train network\n        #\n\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Czas przygotować model sieci, która będzie się uczyła poruszania po środowisku *FrozenLake*, warstwa wejściowa powinna mieć tyle neuronów ile jest możlliwych stanów, warstwa wyjściowa tyle neuronów ile jest możliwych akcji do wykonania:"},{"metadata":{"trusted":true},"cell_type":"code","source":"env = frozenLake(\"8x8\")\n\nstate_size = env.get_number_of_states()\naction_size = len(env.get_possible_actions(None))\nlearning_rate = 0.001\n\nmodel =         \n        #\n        # INSERT CODE HERE to build network\n        #","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":" Czas nauczyć agenta poruszania się po środowisku *FrozenLake*, jako stan przyjmij wektor o liczbie elementów równej liczbie możliwych stanów, z wartością 1 ustawioną w komórce o indeksie równym aktualnemu stanowi, pozostałe elementy mają być wypełnione zerami:"},{"metadata":{"trusted":true},"cell_type":"code","source":"agent = DQNAgent(action_size, learning_rate, model)\n\nagent.epsilon = 0.75\n\ndone = False\nbatch_size = 64\nEPISODES = 10000\ncounter = 0\nfor e in range(EPISODES):\n    summary = []\n    for _ in range(100):\n        total_reward = 0\n        env_state = env.reset()\n    \n        #\n        # INSERT CODE HERE to prepare appropriate format of the state for network\n        #\n        \n        for time in range(1000):\n            action = agent.get_action(state)\n            next_state_env, reward, done, _ = env.step(action)\n            total_reward += reward\n\n            #\n            # INSERT CODE HERE to prepare appropriate format of the next state for network\n            #\n\n            #add to experience memory\n            agent.remember(state, action, reward, next_state, done)\n            state = next_state\n            if done:\n                break\n\n        #\n        # INSERT CODE HERE to train network if in the memory is more samples then size of the batch\n        #\n        \n        summary.append(total_reward)\n    if np.mean(total_reward) > 0.9:\n        print (\"You Win!\")\n        break\n    print(\"epoch #{}\\tmean reward = {:.3f}\\tepsilon = {:.3f}\".format(e, np.mean(summary), agent.epsilon))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Czas przygotować model sieci, która będzie się uczyła poruszania po środowisku *FrozenLakeExtended*, tym razem stan nie jest określany poprzez pojedynczą liczbę, a przez 3 tablice:\n* pierwsza zawierająca informacje o celu,\n* druga zawierająca informacje o dziurach,\n* trzecia zawierająca informację o położeniu gracza."},{"metadata":{"trusted":true},"cell_type":"code","source":"env = frozenLakeExtended(\"4x4\")\n\nstate_size = env.get_number_of_states()\naction_size = len(env.get_possible_actions(None))\nlearning_rate = 0.001\n\nmodel =         \n        #\n        # INSERT CODE HERE to build network\n        #","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":" Czas nauczyć agenta poruszania się po środowisku *FrozenLakeExtended*, jako stan przyjmij wektor składający się ze wszystkich trzech tablic:"},{"metadata":{"trusted":true},"cell_type":"code","source":"agent = DQNAgent(action_size, learning_rate, model)\n\nagent.epsilon = 0.75\n\ndone = False\nbatch_size = 64\nEPISODES = 2000\ncounter = 0\nfor e in range(EPISODES):\n    summary = []\n    for _ in range(100):\n        total_reward = 0\n        env_state = env.reset()\n    \n        #\n        # INSERT CODE HERE to prepare appropriate format of the state for network\n        #\n        \n        for time in range(1000):\n            action = agent.get_action(state)\n            next_state_env, reward, done, _ = env.step(action)\n            total_reward += reward\n\n            #\n            # INSERT CODE HERE to prepare appropriate format of the next state for network\n            #\n\n            #add to experience memory\n            agent.remember(state, action, reward, next_state, done)\n            state = next_state\n            if done:\n                break\n\n        #\n        # INSERT CODE HERE to train network if in the memory is more samples then size of the batch\n        #\n        \n        summary.append(total_reward)\n    if np.mean(total_reward) > 0.9:\n        print (\"You Win!\")\n        break\n    print(\"epoch #{}\\tmean reward = {:.3f}\\tepsilon = {:.3f}\".format(e, np.mean(summary), agent.epsilon))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Czas przygotować model sieci, która będzie się uczyła działania w środowisku [*CartPool*](https://gym.openai.com/envs/CartPole-v0/):"},{"metadata":{"trusted":true},"cell_type":"code","source":"env = gym.make(\"CartPole-v0\").env\nstate_size = env.observation_space.shape[0]\naction_size = env.action_space.n\nlearning_rate = 0.001\n\nmodel =         \n        #\n        # INSERT CODE HERE to build network\n        #","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Czas nauczyć agenta gry w środowisku *CartPool*:"},{"metadata":{"trusted":true},"cell_type":"code","source":"agent = DQNAgent(action_size, learning_rate, model)\n\nagent.epsilon = 0.75\n\ndone = False\nbatch_size = 64\nEPISODES = 1000\ncounter = 0\nfor e in range(EPISODES):\n    summary = []\n    for _ in range(100):\n        total_reward = 0\n        env_state = env.reset()\n    \n        #\n        # INSERT CODE HERE to prepare appropriate format of the state for network\n        #\n        \n        for time in range(1000):\n            action = agent.get_action(state)\n            next_state_env, reward, done, _ = env.step(action)\n            total_reward += reward\n\n            #\n            # INSERT CODE HERE to prepare appropriate format of the next state for network\n            #\n\n            #add to experience memory\n            agent.remember(state, action, reward, next_state, done)\n            state = next_state\n            if done:\n                break\n\n        #\n        # INSERT CODE HERE to train network if in the memory is more samples then size of the batch\n        #\n        \n        summary.append(total_reward)\n    if np.mean(total_reward) > 195:\n        print (\"You Win!\")\n        break\n    print(\"epoch #{}\\tmean reward = {:.3f}\\tepsilon = {:.3f}\".format(e, np.mean(summary), agent.epsilon))","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.6.10","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":4}