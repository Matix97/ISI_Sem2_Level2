{"cells":[{"metadata":{},"cell_type":"markdown","source":"# Laboratorium 5\n\nCelem czwartego laboratorium jest zapoznanie się oraz zaimplementowanie algorytmów głębokiego uczenia aktywnego. Zaimplementowane algorytmy będą testowane z wykorzystaniem środowiska z OpenAI - *CartPole*.\n"},{"metadata":{},"cell_type":"markdown","source":"Dołączenie standardowych bibliotek"},{"metadata":{"trusted":true},"cell_type":"code","source":"from collections import deque\nimport gym\nimport numpy as np\nimport random","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Dołączenie bibliotek do obsługi sieci neuronowych"},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Zadanie 1 - Double Deep Q-Network\n\n<p style='text-align: justify;'>\nCelem ćwiczenie jest zaimplementowanie algorytmu Double Deep Q-Network. Wartoscią oczekiwaną sieci jest:\n\\begin{equation}\n       Q^*(s, a) \\approx r + \\gamma argmax_{a'}Q_\\theta'(s', a') \n\\end{equation}\na wagi pomiędzy sieciami wymieniane są co dziesięć aktualizacji wag sieci sterującej poczynaniami agenta ($Q$).\n</p>"},{"metadata":{"trusted":true},"cell_type":"code","source":"class DDQNAgent:\n    def __init__(self, state_size, action_size):\n        self.state_size = state_size\n        self.action_size = action_size\n        self.memory = deque(maxlen=2000)\n        self.gamma = 0.95    # discount rate\n        self.epsilon = 0.5  # exploration rate\n        self.epsilon_min = 0.01\n        self.epsilon_decay = 0.95\n        self.learning_rate = 0.001\n        self.model = self._build_model()\n        self.target_model = self._build_model()\n        self.update_weights()\n        self.replay_counter = 1\n\n    def remember(self, state, action, reward, next_state, done):\n        #Function adds information to the memory about last action and its results\n        self.memory.append((state, action, reward, next_state, done)) \n\n    def get_action(self, state):\n        \"\"\"\n        Compute the action to take in the current state, including exploration.\n        With probability self.epsilon, we should take a random action.\n            otherwise - the best policy action (self.get_best_action).\n\n        Note: To pick randomly from a list, use random.choice(list).\n              To pick True or False with a given probablity, generate uniform number in [0, 1]\n              and compare it with your probability\n        \"\"\"\n\n        #\n        # INSERT CODE HERE to get action in a given state (according to epsilon greedy algorithm)\n        #        \n        \n        return chosen_action\n\n  \n    def get_best_action(self, state):\n        \"\"\"\n        Compute the best action to take in a state.\n        \"\"\"\n\n        #\n        # INSERT CODE HERE to get best possible action in a given state (remember to break ties randomly)\n        #\n\n        return best_action\n\n    def replay(self, batch_size):\n        \"\"\"\n        Function learn network using randomly selected actions from the memory. \n        First calculates Q value for the next state and choose action with the biggest value.\n        Target value is calculated according to:\n                Q(s,a) := (r + gamma * max_a(Q(s', a)))\n        except the situation when the next action is the last action, in such case Q(s, a) := r.\n        In order to change only those weights responsible for chosing given action, the rest values should be those\n        returned by the network for state state.\n        The network should be trained on batch_size samples.\n        Also every time the function replay is called self.epsilon value should be updated according to equation:\n        self.epsilon *= self.epsilon_decay\n        After each 10 Q Network trainings parameters should be copied to the target Q Network\n        \"\"\"\n        #\n        # INSERT CODE HERE to train network\n        #\n\n    def update_weights(self):\n        \"\"\"copy trained Q Network params to target Q Network\"\"\"\n        #\n        # INSERT CODE HERE to train network\n        #\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Czas przygotować model sieci, która będzie się uczyła działania w środowisku [*CartPool*](https://gym.openai.com/envs/CartPole-v0/):"},{"metadata":{"trusted":true},"cell_type":"code","source":"env = gym.make(\"CartPole-v0\").env\nstate_size = env.observation_space.shape[0]\naction_size = env.action_space.n\nlearning_rate = 0.001\n\nmodel =         \n        #\n        # INSERT CODE HERE to build network\n        #","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Czas nauczyć agenta gry w środowisku *CartPool*:"},{"metadata":{"trusted":true},"cell_type":"code","source":"agent = DDQNAgent(action_size, learning_rate, model)\n\nagent.epsilon = 0.75\n\ndone = False\nbatch_size = 64\nEPISODES = 1000\ncounter = 0\nfor e in range(EPISODES):\n    summary = []\n    for _ in range(100):\n        total_reward = 0\n        env_state = env.reset()\n    \n        #\n        # INSERT CODE HERE to prepare appropriate format of the state for network\n        #\n        \n        for time in range(1000):\n            action = agent.get_action(state)\n            next_state_env, reward, done, _ = env.step(action)\n            total_reward += reward\n\n            #\n            # INSERT CODE HERE to prepare appropriate format of the next state for network\n            #\n\n            #add to experience memory\n            agent.remember(state, action, reward, next_state, done)\n            state = next_state\n            if done:\n                break\n\n        #\n        # INSERT CODE HERE to train network if in the memory is more samples then size of the batch\n        #\n        \n        summary.append(total_reward)\n    if np.mean(total_reward) > 195:\n        print (\"You Win!\")\n        break\n    print(\"epoch #{}\\tmean reward = {:.3f}\\tepsilon = {:.3f}\".format(e, np.mean(summary), agent.epsilon))","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.6.10","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":4}