{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "sAG_ZiiYE0iM",
    "outputId": "207b6814-adf8-48c7-fc26-34d0bce09a33"
   },
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'gym'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-1-f4c2254c4d7e>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m#%tensorflow_version 1.x\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mcollections\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mdeque\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m \u001b[1;32mimport\u001b[0m \u001b[0mgym\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      4\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mrandom\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'gym'"
     ]
    }
   ],
   "source": [
    "#%tensorflow_version 1.x \n",
    "from collections import deque\n",
    "import gym\n",
    "import numpy as np\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "AJhCheKWE2bU",
    "outputId": "9b41c19c-c158-4ad5-cce4-ad4243a203d4"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "#import tensorflow as tf\n",
    "import  keras #tensorflow.keras as\n",
    "from keras import Model\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.optimizers import Adam\n",
    "from keras.utils import to_categorical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "-ESIPj2qF3F2"
   },
   "outputs": [],
   "source": [
    "!cp /content/drive/MyDrive/ISI/env/FrozenLakeMDP.py .\n",
    "!cp /content/drive/MyDrive/ISI/env/FrozenLakeMDPExtended.py ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "KFXvfrRJGBlL"
   },
   "outputs": [],
   "source": [
    "# import FrozenLakeMDP\n",
    "# import FrozenLakeMDPExtended\n",
    "from FrozenLakeMDP import frozenLake\n",
    "from FrozenLakeMDPExtended import frozenLakeExtended"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "D3ggLh63MMMG"
   },
   "outputs": [],
   "source": [
    "import time as tm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "fSn9S5HpGG3C"
   },
   "outputs": [],
   "source": [
    "class DQNAgent:\n",
    "    def __init__(self, action_size, learning_rate, model,get_legal_actions):\n",
    "        self.action_size = action_size\n",
    "        self.memory = deque(maxlen=2000)\n",
    "        self.gamma = 0.995    # discount rate\n",
    "        self.epsilon = 1.0  # exploration rate\n",
    "        self.epsilon_min = 0.01\n",
    "        self.epsilon_decay = 0.95\n",
    "        self.learning_rate = learning_rate\n",
    "        self.model = model\n",
    "        self.get_legal_actions = get_legal_actions\n",
    "\n",
    "    def remember(self, state, action, reward, next_state, done):\n",
    "        #Function adds information to the memory about last action and its results\n",
    "        self.memory.append((state, action, reward, next_state, done)) \n",
    "\n",
    "    def get_action(self, state):\n",
    "        \"\"\"\n",
    "        Compute the action to take in the current state, including exploration.\n",
    "        With probability self.epsilon, we should take a random action.\n",
    "            otherwise - the best policy action (self.get_best_action).\n",
    "\n",
    "        Note: To pick randomly from a list, use random.choice(list).\n",
    "              To pick True or False with a given probablity, generate uniform number in [0, 1]\n",
    "              and compare it with your probability\n",
    "        \"\"\"\n",
    "\n",
    "        #\n",
    "        # INSERT CODE HERE to get action in a given state (according to epsilon greedy algorithm)\n",
    "        #    \n",
    "        possible_actions = self.get_legal_actions(state)  \n",
    "        epsilon = self.epsilon  \n",
    "        if random.random()<epsilon:\n",
    "            chosen_action = random.choice(possible_actions)\n",
    "        else:   \n",
    "            chosen_action = self.get_best_action(state) \n",
    "        return chosen_action\n",
    "\n",
    "  \n",
    "    def get_best_action(self, state):\n",
    "        \"\"\"\n",
    "        Compute the best action to take in a state.\n",
    "        \"\"\"\n",
    "\n",
    "        #\n",
    "        # INSERT CODE HERE to get best possible action in a given state (remember to break ties randomly)\n",
    "        #\n",
    "        best_actions = self.model.predict(state)\n",
    "        #na końcu softmax to weźmy maksa:\n",
    "        #print(\"BEST: \",best_actions)\n",
    "        best_action = np.argmax(best_actions)\n",
    "        best_action = np.argmax(best_action)#s 83  'numpy.int64' object is not iterable  max(best_action[0])  nie działa???\n",
    "        #print(best_action)\n",
    "\n",
    "\n",
    "        return best_action\n",
    "\n",
    "    def replay(self, batch_size):\n",
    "        \"\"\"\n",
    "        Function learn network using randomly selected actions from the memory. \n",
    "        First calculates Q value for the next state and choose action with the biggest value.\n",
    "        Target value is calculated according to:\n",
    "                Q(s,a) := (r + gamma * max_a(Q(s', a)))\n",
    "        except the situation when the next action is the last action, in such case Q(s, a) := r.\n",
    "        In order to change only those weights responsible for chosing given action, the rest values should be those\n",
    "        returned by the network for state state.\n",
    "        The network should be trained on batch_size samples.\n",
    "        \"\"\"\n",
    "        #\n",
    "        # INSERT CODE HERE to train network\n",
    "        #\n",
    "        if batch_size >     len(self.memory):\n",
    "            raise NameError('!!! To BIG batch_size !!! Should be lower than: ',len(self.memory))\n",
    "        #trochę pseudokod jeszcze\n",
    "        # nasz_batch = []\n",
    "        # for i in range(batch_size):\n",
    "        #     nasz_batch.append(random.choice(self.memory)) \n",
    "        nasz_batch = random.sample(self.memory,batch_size)\n",
    "       # x = np.array()\n",
    "       # y = np.array()\n",
    "        x=[]\n",
    "        y=[]\n",
    "        for probka in nasz_batch:#pkt 3 slajd 71/99\n",
    "            #pozostałe akcje to co sięc zwraca\n",
    "            #a wybrana zgodnie ze wzrem\n",
    "            state, action, reward, next_state, done = probka\n",
    "            x.append(state.flatten())\n",
    "            q_pred_prob = self.model.predict(state)\n",
    "            if done:\n",
    "              q_pred_prob[0][action] = reward\n",
    "            else:\n",
    "              #print(\"NEXT STATE: \",next_state)\n",
    "              q_pred_prob[0][action] = reward+ self.gamma*self.get_best_action(next_state)\n",
    "            #print(q_pred_prob)\n",
    "            q_pred_prob = q_pred_prob.flatten()\n",
    "            #print(\"AFETER FLATTEN: \",q_pred_prob)\n",
    "            y.append(q_pred_prob)\n",
    "\n",
    "        self.model.train_on_batch(np.array(x),np.array(y))\n",
    "        self.update_epsilon_value()\n",
    "      \n",
    "        \n",
    "\n",
    "    def update_epsilon_value(self):\n",
    "        #Every each epoch epsilon value should be updated according to equation: \n",
    "        #self.epsilon *= self.epsilon_decay, but the updated value shouldn't be lower then epsilon_min value\n",
    "        temp = self.epsilon*self.epsilon_decay\n",
    "        if temp < self.epsilon_min:\n",
    "            self.epsilon = self.epsilon_min\n",
    "        else:\n",
    "            self.epsilon = temp\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "bEpHxTVFVdkN",
    "outputId": "7f8135ba-b58a-4c3c-c5ef-734f64b14697"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /tensorflow-1.15.2/python3.6/tensorflow_core/python/ops/resource_variable_ops.py:1630: calling BaseResourceVariable.__init__ (from tensorflow.python.ops.resource_variable_ops) with constraint is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "If using Keras pass *_constraint arguments to layers.\n"
     ]
    }
   ],
   "source": [
    "env = frozenLake(\"8x8\")\n",
    "\n",
    "state_size = env.get_number_of_states()\n",
    "action_size = len(env.get_possible_actions(None))\n",
    "learning_rate = 0.001\n",
    "model = [\n",
    "    Dense(64, input_dim=state_size,activation='relu'),\n",
    "    Dense(32,activation='relu'),\n",
    "    #Dropout(0.5),\n",
    "    Dense(action_size,activation='softmax')\n",
    "] \n",
    "model = Sequential(model)\n",
    "model.compile(loss=\"mean_squared_error\",\n",
    "              optimizer=Adam(lr=learning_rate))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "v8hvpjXpMpTz"
   },
   "outputs": [],
   "source": [
    "#!pip install goto\n",
    "#from goto import goto, label\n",
    "agent = DQNAgent(action_size, learning_rate, model,env.get_possible_actions)\n",
    "\n",
    "agent.epsilon = 0.75\n",
    "\n",
    "done = False\n",
    "batch_size = 64\n",
    "EPISODES = 10000\n",
    "counter = 0\n",
    "for e in range(EPISODES):\n",
    "\n",
    "    summary = []\n",
    "    for _ in range(100):\n",
    "        total_reward = 0\n",
    "        env_state = env.reset()\n",
    "        #print(env_state)\n",
    "        #goto .breakall\n",
    "        #\n",
    "        # INSERT CODE HERE to prepare appropriate format of the state for network\n",
    "        #\n",
    "        state = np.array([to_categorical(env_state, num_classes=state_size)])\n",
    "        for time in range(1000):\n",
    "            action = agent.get_action(state)\n",
    "            next_state_env, reward, done, _ = env.step(action)\n",
    "            total_reward += reward\n",
    "\n",
    "            #\n",
    "            # INSERT CODE HERE to prepare appropriate format of the next state for network\n",
    "            #\n",
    "            next_state = np.array([to_categorical(next_state_env, num_classes=state_size)])\n",
    "            #add to experience memory\n",
    "            agent.remember(state, action, reward, next_state, done)\n",
    "            state = next_state\n",
    "            if done:\n",
    "                break\n",
    "\n",
    "        #\n",
    "        # INSERT CODE HERE to train network if in the memory is more samples then size of the batch\n",
    "        #\n",
    "            if len(agent.memory) > batch_size:\n",
    "                agent.replay(64)\n",
    "        \n",
    "        summary.append(total_reward)\n",
    "    print(\"epoch #{}\\tmean reward = {:.3f}\\tepsilon = {:.3f}\".format(e, np.mean(summary), agent.epsilon))\n",
    "    if np.mean(summary) > 0.9:\n",
    "        print (\"You Win!\")\n",
    "        break\n",
    "#label .breakall"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Ylk_TZQtGVMp",
    "outputId": "f885d7d0-fd0f-42f3-d546-e719aec08859"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "64\n",
      "4\n"
     ]
    }
   ],
   "source": [
    "env = frozenLake(\"8x8\")\n",
    "\n",
    "state_size = env.get_number_of_states()\n",
    "action_size = len(env.get_possible_actions(None))\n",
    "learning_rate = 0.001\n",
    "print(state_size)\n",
    "print(action_size)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "rcBjOwmUHDtD"
   },
   "outputs": [],
   "source": [
    "env = frozenLake(\"8x8\")\n",
    "\n",
    "state_size = env.get_number_of_states()\n",
    "action_size = len(env.get_possible_actions(None))\n",
    "learning_rate = 0.001\n",
    "model = Sequential()\n",
    "model.add(Dense(64, input_dim=state_size, activation=\"relu\"))\n",
    "model.add(Dense(32, activation=\"relu\"))\n",
    "model.add(Dense(16, activation=\"relu\"))\n",
    "model.add(Dense(action_size))  # wyjście\n",
    "model.compile(loss=\"mean_squared_error\",\n",
    "              optimizer=Adam(lr=learning_rate))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "24TERbEiLse5",
    "outputId": "56c754d5-555c-4b22-8de1-6f6cf1501147"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BEST:  [[-0.03166572  0.06883486  0.08737504  0.05276528]]\n",
      "0\n",
      "BEST:  [[-0.03166572  0.06883486  0.08737504  0.05276528]]\n",
      "0\n",
      "BEST:  [[-0.03166572  0.06883486  0.08737504  0.05276528]]\n",
      "0\n",
      "BEST:  [[0.02089614 0.07757111 0.02935774 0.03605977]]\n",
      "0\n",
      "BEST:  [[-0.03166572  0.06883486  0.08737504  0.05276528]]\n",
      "0\n",
      "BEST:  [[0.02089614 0.07757111 0.02935774 0.03605977]]\n",
      "0\n",
      "BEST:  [[0.02089614 0.07757111 0.02935774 0.03605977]]\n",
      "0\n",
      "BEST:  [[0.01578376 0.07555803 0.04652528 0.10977717]]\n",
      "0\n",
      "BEST:  [[-0.03166572  0.06883486  0.08737504  0.05276528]]\n",
      "0\n",
      "BEST:  [[-0.03166572  0.06883486  0.08737504  0.05276528]]\n",
      "0\n",
      "BEST:  [[-0.03166572  0.06883486  0.08737504  0.05276528]]\n",
      "0\n",
      "BEST:  [[-0.03166572  0.06883486  0.08737504  0.05276528]]\n",
      "0\n",
      "BEST:  [[0.02089614 0.07757111 0.02935774 0.03605977]]\n",
      "0\n",
      "BEST:  [[0.02089614 0.07757111 0.02935774 0.03605977]]\n",
      "0\n",
      "BEST:  [[0.02089614 0.07757111 0.02935774 0.03605977]]\n",
      "0\n",
      "BEST:  [[0.02089614 0.07757111 0.02935774 0.03605977]]\n",
      "0\n",
      "BEST:  [[0.02089614 0.07757111 0.02935774 0.03605977]]\n",
      "0\n",
      "BEST:  [[0.02089614 0.07757111 0.02935774 0.03605977]]\n",
      "0\n",
      "BEST:  [[-0.00070718  0.05650281  0.06020997  0.02160836]]\n",
      "0\n",
      "BEST:  [[-0.00070718  0.05650281  0.06020997  0.02160836]]\n",
      "0\n",
      "BEST:  [[0.02089614 0.07757111 0.02935774 0.03605977]]\n",
      "0\n",
      "NEXT STATE:  [[0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "  0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "  0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]]\n",
      "BEST:  [[0.01578376 0.07555803 0.04652528 0.10977717]]\n",
      "0\n",
      "[[-0.00070718  0.          0.06020997  0.02160836]]\n",
      "AFETER FLATTEN:  [-0.00070718  0.          0.06020997  0.02160836]\n",
      "NEXT STATE:  [[1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "  0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "  0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]]\n",
      "BEST:  [[0.02089614 0.07757111 0.02935774 0.03605977]]\n",
      "0\n",
      "[[0.         0.07757111 0.02935774 0.03605977]]\n",
      "AFETER FLATTEN:  [0.         0.07757111 0.02935774 0.03605977]\n",
      "NEXT STATE:  [[1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "  0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "  0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]]\n",
      "BEST:  [[0.02089614 0.07757111 0.02935774 0.03605977]]\n",
      "0\n",
      "[[0.         0.07757111 0.02935774 0.03605977]]\n",
      "AFETER FLATTEN:  [0.         0.07757111 0.02935774 0.03605977]\n",
      "NEXT STATE:  [[1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "  0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "  0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]]\n",
      "BEST:  [[0.02089614 0.07757111 0.02935774 0.03605977]]\n",
      "0\n",
      "[[0.         0.05650281 0.06020997 0.02160836]]\n",
      "AFETER FLATTEN:  [0.         0.05650281 0.06020997 0.02160836]\n",
      "NEXT STATE:  [[0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "  0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "  0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]]\n",
      "BEST:  [[-0.03166572  0.06883486  0.08737504  0.05276528]]\n",
      "0\n",
      "[[0.         0.06883486 0.08737504 0.05276528]]\n",
      "AFETER FLATTEN:  [0.         0.06883486 0.08737504 0.05276528]\n",
      "NEXT STATE:  [[1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "  0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "  0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]]\n",
      "BEST:  [[0.02089614 0.07757111 0.02935774 0.03605977]]\n",
      "0\n",
      "[[0.         0.07757111 0.02935774 0.03605977]]\n",
      "AFETER FLATTEN:  [0.         0.07757111 0.02935774 0.03605977]\n",
      "NEXT STATE:  [[1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "  0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "  0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]]\n",
      "BEST:  [[0.02089614 0.07757111 0.02935774 0.03605977]]\n",
      "0\n",
      "[[0.02089614 0.07757111 0.02935774 0.        ]]\n",
      "AFETER FLATTEN:  [0.02089614 0.07757111 0.02935774 0.        ]\n",
      "NEXT STATE:  [[1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "  0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "  0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]]\n",
      "BEST:  [[0.02089614 0.07757111 0.02935774 0.03605977]]\n",
      "0\n",
      "[[0.         0.05650281 0.06020997 0.02160836]]\n",
      "AFETER FLATTEN:  [0.         0.05650281 0.06020997 0.02160836]\n",
      "NEXT STATE:  [[1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "  0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "  0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]]\n",
      "BEST:  [[0.02089614 0.07757111 0.02935774 0.03605977]]\n",
      "0\n",
      "[[0.         0.07757111 0.02935774 0.03605977]]\n",
      "AFETER FLATTEN:  [0.         0.07757111 0.02935774 0.03605977]\n",
      "NEXT STATE:  [[0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "  0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "  0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]]\n",
      "BEST:  [[-0.03166572  0.06883486  0.08737504  0.05276528]]\n",
      "0\n",
      "[[0.         0.07555803 0.04652528 0.10977717]]\n",
      "AFETER FLATTEN:  [0.         0.07555803 0.04652528 0.10977717]\n",
      "NEXT STATE:  [[1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "  0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "  0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]]\n",
      "BEST:  [[0.02089614 0.07757111 0.02935774 0.03605977]]\n",
      "0\n",
      "[[0.         0.07757111 0.02935774 0.03605977]]\n",
      "AFETER FLATTEN:  [0.         0.07757111 0.02935774 0.03605977]\n",
      "NEXT STATE:  [[0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "  0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "  0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]]\n",
      "BEST:  [[-0.03166572  0.06883486  0.08737504  0.05276528]]\n",
      "0\n",
      "[[0.02089614 0.         0.02935774 0.03605977]]\n",
      "AFETER FLATTEN:  [0.02089614 0.         0.02935774 0.03605977]\n",
      "NEXT STATE:  [[1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "  0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "  0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]]\n",
      "BEST:  [[0.02089614 0.07757111 0.02935774 0.03605977]]\n",
      "0\n",
      "[[0.         0.07757111 0.02935774 0.03605977]]\n",
      "AFETER FLATTEN:  [0.         0.07757111 0.02935774 0.03605977]\n",
      "NEXT STATE:  [[1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "  0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "  0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]]\n",
      "BEST:  [[0.02089614 0.07757111 0.02935774 0.03605977]]\n",
      "0\n",
      "[[-0.03166572  0.06883486  0.08737504  0.        ]]\n",
      "AFETER FLATTEN:  [-0.03166572  0.06883486  0.08737504  0.        ]\n",
      "NEXT STATE:  [[0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "  0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "  0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]]\n",
      "BEST:  [[-0.03166572  0.06883486  0.08737504  0.05276528]]\n",
      "0\n",
      "[[0.02089614 0.         0.02935774 0.03605977]]\n",
      "AFETER FLATTEN:  [0.02089614 0.         0.02935774 0.03605977]\n",
      "NEXT STATE:  [[1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "  0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "  0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]]\n",
      "BEST:  [[0.02089614 0.07757111 0.02935774 0.03605977]]\n",
      "0\n",
      "[[0.         0.07757111 0.02935774 0.03605977]]\n",
      "AFETER FLATTEN:  [0.         0.07757111 0.02935774 0.03605977]\n",
      "NEXT STATE:  [[0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "  0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "  0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]]\n",
      "BEST:  [[-0.00070718  0.05650281  0.06020997  0.02160836]]\n",
      "0\n",
      "[[0.02089614 0.07757111 0.         0.03605977]]\n",
      "AFETER FLATTEN:  [0.02089614 0.07757111 0.         0.03605977]\n",
      "NEXT STATE:  [[1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "  0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "  0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]]\n",
      "BEST:  [[0.02089614 0.07757111 0.02935774 0.03605977]]\n",
      "0\n",
      "[[0.         0.07757111 0.02935774 0.03605977]]\n",
      "AFETER FLATTEN:  [0.         0.07757111 0.02935774 0.03605977]\n",
      "NEXT STATE:  [[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0.\n",
      "  0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "  0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]]\n",
      "BEST:  [[0.01603807 0.04529386 0.00655436 0.01687049]]\n",
      "0\n",
      "[[-0.03166572  0.          0.08737504  0.05276528]]\n",
      "AFETER FLATTEN:  [-0.03166572  0.          0.08737504  0.05276528]\n",
      "NEXT STATE:  [[0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "  0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "  0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]]\n",
      "BEST:  [[-0.03166572  0.06883486  0.08737504  0.05276528]]\n",
      "0\n",
      "[[0.         0.06883486 0.08737504 0.05276528]]\n",
      "AFETER FLATTEN:  [0.         0.06883486 0.08737504 0.05276528]\n",
      "NEXT STATE:  [[1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "  0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "  0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]]\n",
      "BEST:  [[0.02089614 0.07757111 0.02935774 0.03605977]]\n",
      "0\n",
      "[[0.         0.07757111 0.02935774 0.03605977]]\n",
      "AFETER FLATTEN:  [0.         0.07757111 0.02935774 0.03605977]\n",
      "NEXT STATE:  [[0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "  0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "  0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]]\n",
      "BEST:  [[-0.03166572  0.06883486  0.08737504  0.05276528]]\n",
      "0\n",
      "[[0.         0.06883486 0.08737504 0.05276528]]\n",
      "AFETER FLATTEN:  [0.         0.06883486 0.08737504 0.05276528]\n",
      "NEXT STATE:  [[1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "  0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "  0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]]\n",
      "BEST:  [[0.02089614 0.07757111 0.02935774 0.03605977]]\n",
      "0\n",
      "[[0.         0.07757111 0.02935774 0.03605977]]\n",
      "AFETER FLATTEN:  [0.         0.07757111 0.02935774 0.03605977]\n",
      "NEXT STATE:  [[0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "  0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "  0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]]\n",
      "BEST:  [[0.01578376 0.07555803 0.04652528 0.10977717]]\n",
      "0\n",
      "[[-0.00070718  0.          0.06020997  0.02160836]]\n",
      "AFETER FLATTEN:  [-0.00070718  0.          0.06020997  0.02160836]\n",
      "NEXT STATE:  [[0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "  0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "  0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]]\n",
      "BEST:  [[-0.00070718  0.05650281  0.06020997  0.02160836]]\n",
      "0\n",
      "[[0.02089614 0.07757111 0.         0.03605977]]\n",
      "AFETER FLATTEN:  [0.02089614 0.07757111 0.         0.03605977]\n",
      "NEXT STATE:  [[0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "  0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "  0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]]\n",
      "BEST:  [[-0.03166572  0.06883486  0.08737504  0.05276528]]\n",
      "0\n",
      "[[0.02089614 0.         0.02935774 0.03605977]]\n",
      "AFETER FLATTEN:  [0.02089614 0.         0.02935774 0.03605977]\n",
      "NEXT STATE:  [[1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "  0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "  0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]]\n",
      "BEST:  [[0.02089614 0.07757111 0.02935774 0.03605977]]\n",
      "0\n",
      "[[0.         0.07757111 0.02935774 0.03605977]]\n",
      "AFETER FLATTEN:  [0.         0.07757111 0.02935774 0.03605977]\n",
      "NEXT STATE:  [[1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "  0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "  0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]]\n",
      "BEST:  [[0.02089614 0.07757111 0.02935774 0.03605977]]\n",
      "0\n",
      "[[-0.03166572  0.06883486  0.08737504  0.        ]]\n",
      "AFETER FLATTEN:  [-0.03166572  0.06883486  0.08737504  0.        ]\n",
      "NEXT STATE:  [[1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "  0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "  0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]]\n",
      "BEST:  [[0.02089614 0.07757111 0.02935774 0.03605977]]\n",
      "0\n",
      "[[-0.03166572  0.06883486  0.08737504  0.        ]]\n",
      "AFETER FLATTEN:  [-0.03166572  0.06883486  0.08737504  0.        ]\n",
      "NEXT STATE:  [[0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "  0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "  0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]]\n",
      "BEST:  [[-0.03166572  0.06883486  0.08737504  0.05276528]]\n",
      "0\n",
      "[[0.         0.06883486 0.08737504 0.05276528]]\n",
      "AFETER FLATTEN:  [0.         0.06883486 0.08737504 0.05276528]\n",
      "NEXT STATE:  [[1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "  0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "  0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]]\n",
      "BEST:  [[0.02089614 0.07757111 0.02935774 0.03605977]]\n",
      "0\n",
      "[[-0.03166572  0.06883486  0.08737504  0.        ]]\n",
      "AFETER FLATTEN:  [-0.03166572  0.06883486  0.08737504  0.        ]\n",
      "NEXT STATE:  [[0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "  0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "  0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]]\n",
      "BEST:  [[-0.03166572  0.06883486  0.08737504  0.05276528]]\n",
      "0\n",
      "[[0.         0.06883486 0.08737504 0.05276528]]\n",
      "AFETER FLATTEN:  [0.         0.06883486 0.08737504 0.05276528]\n",
      "NEXT STATE:  [[1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "  0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "  0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]]\n",
      "BEST:  [[0.02089614 0.07757111 0.02935774 0.03605977]]\n",
      "0\n",
      "[[0.         0.08079766 0.09011246 0.04086079]]\n",
      "AFETER FLATTEN:  [0.         0.08079766 0.09011246 0.04086079]\n",
      "NEXT STATE:  [[1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "  0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "  0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]]\n",
      "BEST:  [[0.02089614 0.07757111 0.02935774 0.03605977]]\n",
      "0\n",
      "[[-0.03166572  0.06883486  0.08737504  0.        ]]\n",
      "AFETER FLATTEN:  [-0.03166572  0.06883486  0.08737504  0.        ]\n",
      "NEXT STATE:  [[1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "  0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "  0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]]\n",
      "BEST:  [[0.02089614 0.07757111 0.02935774 0.03605977]]\n",
      "0\n",
      "[[0.         0.05650281 0.06020997 0.02160836]]\n",
      "AFETER FLATTEN:  [0.         0.05650281 0.06020997 0.02160836]\n",
      "NEXT STATE:  [[0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "  0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "  0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]]\n",
      "BEST:  [[-0.00070718  0.05650281  0.06020997  0.02160836]]\n",
      "0\n",
      "[[0.01578376 0.07555803 0.04652528 0.        ]]\n",
      "AFETER FLATTEN:  [0.01578376 0.07555803 0.04652528 0.        ]\n",
      "NEXT STATE:  [[1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "  0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "  0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]]\n",
      "BEST:  [[0.02089614 0.07757111 0.02935774 0.03605977]]\n",
      "0\n",
      "[[0.         0.07757111 0.02935774 0.03605977]]\n",
      "AFETER FLATTEN:  [0.         0.07757111 0.02935774 0.03605977]\n",
      "NEXT STATE:  [[0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "  0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "  0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]]\n",
      "BEST:  [[-0.00070718  0.05650281  0.06020997  0.02160836]]\n",
      "0\n",
      "[[0.02089614 0.07757111 0.         0.03605977]]\n",
      "AFETER FLATTEN:  [0.02089614 0.07757111 0.         0.03605977]\n",
      "NEXT STATE:  [[1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "  0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "  0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]]\n",
      "BEST:  [[0.02089614 0.07757111 0.02935774 0.03605977]]\n",
      "0\n",
      "[[0.         0.07757111 0.02935774 0.03605977]]\n",
      "AFETER FLATTEN:  [0.         0.07757111 0.02935774 0.03605977]\n",
      "NEXT STATE:  [[1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "  0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "  0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]]\n",
      "BEST:  [[0.02089614 0.07757111 0.02935774 0.03605977]]\n",
      "0\n",
      "[[-0.03166572  0.06883486  0.08737504  0.        ]]\n",
      "AFETER FLATTEN:  [-0.03166572  0.06883486  0.08737504  0.        ]\n",
      "NEXT STATE:  [[0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "  0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "  0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]]\n",
      "BEST:  [[0.01578376 0.07555803 0.04652528 0.10977717]]\n",
      "0\n",
      "[[-0.03166572  0.06883486  0.          0.05276528]]\n",
      "AFETER FLATTEN:  [-0.03166572  0.06883486  0.          0.05276528]\n",
      "NEXT STATE:  [[0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "  0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "  0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]]\n",
      "BEST:  [[-0.03166572  0.06883486  0.08737504  0.05276528]]\n",
      "0\n",
      "[[0.         0.06883486 0.08737504 0.05276528]]\n",
      "AFETER FLATTEN:  [0.         0.06883486 0.08737504 0.05276528]\n",
      "NEXT STATE:  [[0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "  0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "  0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]]\n",
      "BEST:  [[-0.03166572  0.06883486  0.08737504  0.05276528]]\n",
      "0\n",
      "[[0.         0.06883486 0.08737504 0.05276528]]\n",
      "AFETER FLATTEN:  [0.         0.06883486 0.08737504 0.05276528]\n",
      "NEXT STATE:  [[0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "  0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "  0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]]\n",
      "BEST:  [[-0.00070718  0.05650281  0.06020997  0.02160836]]\n",
      "0\n",
      "[[0.02089614 0.07757111 0.         0.03605977]]\n",
      "AFETER FLATTEN:  [0.02089614 0.07757111 0.         0.03605977]\n",
      "NEXT STATE:  [[1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "  0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "  0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]]\n",
      "BEST:  [[0.02089614 0.07757111 0.02935774 0.03605977]]\n",
      "0\n",
      "[[0.         0.07757111 0.02935774 0.03605977]]\n",
      "AFETER FLATTEN:  [0.         0.07757111 0.02935774 0.03605977]\n",
      "NEXT STATE:  [[1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "  0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "  0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]]\n",
      "BEST:  [[0.02089614 0.07757111 0.02935774 0.03605977]]\n",
      "0\n",
      "[[0.         0.07757111 0.02935774 0.03605977]]\n",
      "AFETER FLATTEN:  [0.         0.07757111 0.02935774 0.03605977]\n",
      "NEXT STATE:  [[0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "  0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "  0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]]\n",
      "BEST:  [[-0.03166572  0.06883486  0.08737504  0.05276528]]\n",
      "0\n",
      "[[0.         0.06883486 0.08737504 0.05276528]]\n",
      "AFETER FLATTEN:  [0.         0.06883486 0.08737504 0.05276528]\n",
      "NEXT STATE:  [[1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "  0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "  0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]]\n",
      "BEST:  [[0.02089614 0.07757111 0.02935774 0.03605977]]\n",
      "0\n",
      "[[0.         0.07757111 0.02935774 0.03605977]]\n",
      "AFETER FLATTEN:  [0.         0.07757111 0.02935774 0.03605977]\n",
      "NEXT STATE:  [[1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "  0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "  0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]]\n",
      "BEST:  [[0.02089614 0.07757111 0.02935774 0.03605977]]\n",
      "0\n",
      "[[0.02089614 0.07757111 0.02935774 0.        ]]\n",
      "AFETER FLATTEN:  [0.02089614 0.07757111 0.02935774 0.        ]\n",
      "NEXT STATE:  [[0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "  0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "  0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]]\n",
      "BEST:  [[-0.03166572  0.06883486  0.08737504  0.05276528]]\n",
      "0\n",
      "[[0.02089614 0.         0.02935774 0.03605977]]\n",
      "AFETER FLATTEN:  [0.02089614 0.         0.02935774 0.03605977]\n",
      "NEXT STATE:  [[1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "  0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "  0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]]\n",
      "BEST:  [[0.02089614 0.07757111 0.02935774 0.03605977]]\n",
      "0\n",
      "[[0.         0.07757111 0.02935774 0.03605977]]\n",
      "AFETER FLATTEN:  [0.         0.07757111 0.02935774 0.03605977]\n",
      "NEXT STATE:  [[1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "  0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "  0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]]\n",
      "BEST:  [[0.02089614 0.07757111 0.02935774 0.03605977]]\n",
      "0\n",
      "[[0.         0.07757111 0.02935774 0.03605977]]\n",
      "AFETER FLATTEN:  [0.         0.07757111 0.02935774 0.03605977]\n",
      "NEXT STATE:  [[1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "  0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "  0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]]\n",
      "BEST:  [[0.02089614 0.07757111 0.02935774 0.03605977]]\n",
      "0\n",
      "[[0.         0.05650281 0.06020997 0.02160836]]\n",
      "AFETER FLATTEN:  [0.         0.05650281 0.06020997 0.02160836]\n",
      "NEXT STATE:  [[0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "  0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "  0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]]\n",
      "BEST:  [[-0.00070718  0.05650281  0.06020997  0.02160836]]\n",
      "0\n",
      "[[0.02089614 0.07757111 0.         0.03605977]]\n",
      "AFETER FLATTEN:  [0.02089614 0.07757111 0.         0.03605977]\n",
      "NEXT STATE:  [[1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "  0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "  0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]]\n",
      "BEST:  [[0.02089614 0.07757111 0.02935774 0.03605977]]\n",
      "0\n",
      "[[0.         0.07757111 0.02935774 0.03605977]]\n",
      "AFETER FLATTEN:  [0.         0.07757111 0.02935774 0.03605977]\n",
      "NEXT STATE:  [[0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "  0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "  0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]]\n",
      "BEST:  [[-0.00070718  0.05650281  0.06020997  0.02160836]]\n",
      "0\n",
      "[[-0.00070718  0.05650281  0.06020997  0.        ]]\n",
      "AFETER FLATTEN:  [-0.00070718  0.05650281  0.06020997  0.        ]\n",
      "NEXT STATE:  [[1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "  0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "  0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]]\n",
      "BEST:  [[0.02089614 0.07757111 0.02935774 0.03605977]]\n",
      "0\n",
      "[[0.02089614 0.07757111 0.02935774 0.        ]]\n",
      "AFETER FLATTEN:  [0.02089614 0.07757111 0.02935774 0.        ]\n",
      "NEXT STATE:  [[0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "  0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "  0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]]\n",
      "BEST:  [[-0.03166572  0.06883486  0.08737504  0.05276528]]\n",
      "0\n",
      "[[0.02089614 0.         0.02935774 0.03605977]]\n",
      "AFETER FLATTEN:  [0.02089614 0.         0.02935774 0.03605977]\n",
      "NEXT STATE:  [[1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "  0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "  0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]]\n",
      "BEST:  [[0.02089614 0.07757111 0.02935774 0.03605977]]\n",
      "0\n",
      "[[-0.03166572  0.06883486  0.08737504  0.        ]]\n",
      "AFETER FLATTEN:  [-0.03166572  0.06883486  0.08737504  0.        ]\n",
      "NEXT STATE:  [[0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "  0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "  0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]]\n",
      "BEST:  [[-0.03166572  0.06883486  0.08737504  0.05276528]]\n",
      "0\n",
      "[[0.         0.06883486 0.08737504 0.05276528]]\n",
      "AFETER FLATTEN:  [0.         0.06883486 0.08737504 0.05276528]\n",
      "NEXT STATE:  [[0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "  0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "  0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]]\n",
      "BEST:  [[-0.03166572  0.06883486  0.08737504  0.05276528]]\n",
      "0\n",
      "[[0.         0.06883486 0.08737504 0.05276528]]\n",
      "AFETER FLATTEN:  [0.         0.06883486 0.08737504 0.05276528]\n",
      "NEXT STATE:  [[1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "  0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "  0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]]\n",
      "BEST:  [[0.02089614 0.07757111 0.02935774 0.03605977]]\n",
      "0\n",
      "[[0.         0.07757111 0.02935774 0.03605977]]\n",
      "AFETER FLATTEN:  [0.         0.07757111 0.02935774 0.03605977]\n",
      "NEXT STATE:  [[0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "  0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "  0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]]\n",
      "BEST:  [[-0.03166572  0.06883486  0.08737504  0.05276528]]\n",
      "0\n",
      "[[0.01603807 0.04529386 0.00655436 0.        ]]\n",
      "AFETER FLATTEN:  [0.01603807 0.04529386 0.00655436 0.        ]\n",
      "NEXT STATE:  [[0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "  0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "  0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]]\n",
      "BEST:  [[-0.03166572  0.06883486  0.08737504  0.05276528]]\n",
      "0\n",
      "[[0.         0.07555803 0.04652528 0.10977717]]\n",
      "AFETER FLATTEN:  [0.         0.07555803 0.04652528 0.10977717]\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "ignored",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-39-94ad4be0bee8>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     37\u001b[0m         \u001b[0;31m#\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     38\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0magent\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmemory\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 39\u001b[0;31m                 \u001b[0magent\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreplay\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m64\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     40\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     41\u001b[0m         \u001b[0msummary\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtotal_reward\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-37-338299f12916>\u001b[0m in \u001b[0;36mreplay\u001b[0;34m(self, batch_size)\u001b[0m\n\u001b[1;32m     98\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     99\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_on_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 100\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate_epsilon_value\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    101\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    102\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: update_epsilon_value() takes 0 positional arguments but 1 was given"
     ]
    }
   ],
   "source": [
    "agent = DQNAgent(action_size, learning_rate, model,env.get_possible_actions)\n",
    "\n",
    "agent.epsilon = 0.75\n",
    "\n",
    "done = False\n",
    "batch_size = 64\n",
    "EPISODES = 10000\n",
    "counter = 0\n",
    "for e in range(EPISODES):\n",
    "\n",
    "    summary = []\n",
    "    for _ in range(100):\n",
    "        total_reward = 0\n",
    "        env_state = env.reset()\n",
    "    \n",
    "        #\n",
    "        # INSERT CODE HERE to prepare appropriate format of the state for network\n",
    "        #\n",
    "        \n",
    "        for time in range(1000):\n",
    "            action = agent.get_action(state)\n",
    "            next_state_env, reward, done, _ = env.step(action)\n",
    "            total_reward += reward\n",
    "\n",
    "            #\n",
    "            # INSERT CODE HERE to prepare appropriate format of the next state for network\n",
    "            #\n",
    "            next_state = np.array([to_categorical(next_state_env, num_classes=state_size)])\n",
    "            #add to experience memory\n",
    "            agent.remember(state, action, reward, next_state, done)\n",
    "            state = next_state\n",
    "            if done:\n",
    "                break\n",
    "\n",
    "        #\n",
    "        # INSERT CODE HERE to train network if in the memory is more samples then size of the batch\n",
    "        #\n",
    "            if len(agent.memory) > batch_size:\n",
    "                agent.replay(64)\n",
    "        \n",
    "        summary.append(total_reward)\n",
    "    print(\"epoch #{}\\tmean reward = {:.3f}\\tepsilon = {:.3f}\".format(e, np.mean(summary), agent.epsilon))\n",
    "    if np.mean(summary) > 0.9:\n",
    "        print (\"You Win!\")\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "HC-AIXl2RpDt"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "ISI_Lab4.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
